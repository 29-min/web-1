"""
ë¸”ë¡œê·¸/ì›¹í˜ì´ì§€ì—ì„œ ì½˜í…ì¸  ì¶”ì¶œí•˜ëŠ” ëª¨ë“ˆ
"""
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse
from typing import Optional
from dataclasses import dataclass


@dataclass
class ScrapedContent:
    """ì¶”ì¶œëœ ì½˜í…ì¸ """
    url: str
    title: str
    content: str
    source: str  # ë¸”ë¡œê·¸ í”Œë«í¼ (naver, tistory, etc.)


class BlogScraper:
    """ë¸”ë¡œê·¸ ì½˜í…ì¸  ìŠ¤í¬ë˜í¼"""
    
    HEADERS = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }
    
    # í”Œë«í¼ë³„ ë³¸ë¬¸ ì„ íƒì
    PLATFORM_SELECTORS = {
        "naver": [
            ".se-main-container",  # ìŠ¤ë§ˆíŠ¸ì—ë””í„° 3.0
            ".post-view",
            "#postViewArea",
            ".sect_dsc",
        ],
        "tistory": [
            ".entry-content",
            ".article-view",
            "#article-view",
            ".post-content",
        ],
        "velog": [
            ".atom-one",
            "div[class*='sc-']",  # styled-components
        ],
        "brunch": [
            ".wrap_body",
            ".article_view",
        ],
        "medium": [
            "article",
            ".postArticle-content",
        ],
        "default": [
            "article",
            "main",
            ".post-content",
            ".entry-content",
            ".article-content",
            ".content",
        ]
    }
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update(self.HEADERS)
    
    def _detect_platform(self, url: str) -> str:
        """URLì—ì„œ ë¸”ë¡œê·¸ í”Œë«í¼ ê°ì§€"""
        domain = urlparse(url).netloc.lower()
        
        if "blog.naver" in domain or "m.blog.naver" in domain:
            return "naver"
        elif "tistory" in domain:
            return "tistory"
        elif "velog.io" in domain:
            return "velog"
        elif "brunch.co.kr" in domain:
            return "brunch"
        elif "medium.com" in domain:
            return "medium"
        else:
            return "default"
    
    def _get_naver_blog_content(self, url: str) -> Optional[str]:
        """ë„¤ì´ë²„ ë¸”ë¡œê·¸ iframe ì²˜ë¦¬"""
        try:
            response = self.session.get(url)
            soup = BeautifulSoup(response.text, "html.parser")
            
            # iframe URL ì¶”ì¶œ
            iframe = soup.find("iframe", id="mainFrame")
            if iframe and iframe.get("src"):
                iframe_url = "https://blog.naver.com" + iframe["src"]
                response = self.session.get(iframe_url)
                return response.text
            
            return response.text
        except Exception:
            return None
    
    def _extract_content(self, html: str, platform: str) -> tuple[str, str]:
        """HTMLì—ì„œ ì œëª©ê³¼ ë³¸ë¬¸ ì¶”ì¶œ"""
        soup = BeautifulSoup(html, "html.parser")
        
        # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
        for tag in soup.find_all(["script", "style", "nav", "header", "footer", "aside", "iframe"]):
            tag.decompose()
        
        # ì œëª© ì¶”ì¶œ
        title = ""
        title_tag = soup.find("h1") or soup.find("title")
        if title_tag:
            title = title_tag.get_text(strip=True)
        
        # ë³¸ë¬¸ ì¶”ì¶œ - í”Œë«í¼ë³„ ì„ íƒì ì‹œë„
        selectors = self.PLATFORM_SELECTORS.get(platform, self.PLATFORM_SELECTORS["default"])
        content = ""
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                content = element.get_text(separator="\n", strip=True)
                if len(content) > 100:  # ì¶©ë¶„í•œ ì½˜í…ì¸ ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
                    break
        
        # ì„ íƒìë¡œ ëª» ì°¾ìœ¼ë©´ bodyì—ì„œ ì¶”ì¶œ
        if not content or len(content) < 100:
            body = soup.find("body")
            if body:
                content = body.get_text(separator="\n", strip=True)
        
        # í…ìŠ¤íŠ¸ ì •ë¦¬
        lines = [line.strip() for line in content.split("\n") if line.strip()]
        content = "\n".join(lines)
        
        return title, content
    
    def scrape(self, url: str) -> ScrapedContent:
        """URLì—ì„œ ì½˜í…ì¸  ì¶”ì¶œ"""
        platform = self._detect_platform(url)
        
        try:
            # ë„¤ì´ë²„ ë¸”ë¡œê·¸ëŠ” íŠ¹ë³„ ì²˜ë¦¬
            if platform == "naver":
                html = self._get_naver_blog_content(url)
            else:
                response = self.session.get(url, timeout=10)
                response.raise_for_status()
                html = response.text
            
            if not html:
                raise ValueError("í˜ì´ì§€ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            title, content = self._extract_content(html, platform)
            
            if not content or len(content) < 50:
                raise ValueError("ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í˜ì´ì§€ êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            
            return ScrapedContent(
                url=url,
                title=title,
                content=content[:5000],  # ìµœëŒ€ 5000ìë¡œ ì œí•œ
                source=platform
            )
            
        except requests.RequestException as e:
            raise ValueError(f"í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {e}")


def main():
    """í…ŒìŠ¤íŠ¸"""
    scraper = BlogScraper()
    
    # í…ŒìŠ¤íŠ¸ URL (ì‹¤ì œ í…ŒìŠ¤íŠ¸ì‹œ ë³€ê²½)
    test_url = input("ë¸”ë¡œê·¸ URLì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
    
    try:
        result = scraper.scrape(test_url)
        print(f"\nğŸ“Œ í”Œë«í¼: {result.source}")
        print(f"ğŸ“Œ ì œëª©: {result.title}")
        print(f"\nğŸ“Œ ë³¸ë¬¸ (ì²˜ìŒ 500ì):")
        print(result.content[:500])
        print(f"\n... (ì´ {len(result.content)}ì)")
    except ValueError as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")


if __name__ == "__main__":
    main()